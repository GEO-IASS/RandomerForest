\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Breiman2001}
\citation{Delgado2014}
\citation{Caruana2008}
\citation{Heath1993}
\citation{Heath1993}
\citation{Menze2011}
\citation{Heath1993}
\citation{Tan2005}
\citation{Ho1998}
\citation{Rodriguez2006}
\citation{Menze2011}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Candes2009}
\citation{Li2006}
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear Threshold Forests}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Randomer Forests}{2}{section.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Psuedocode for Linear Threshold Forests, which generalizes a wide range of previously proposed decision forests.}}{3}{algorithm.1}}
\newlabel{pseudo}{{1}{3}{Linear Threshold Forests}{algorithm.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Scatter plot of parallel cigars in two dimensions. 1000 samples are obtained from multivariate Gaussians, where $\mu _j=[j, 0]$, for $j \in \{-1,1\}$, and $\Sigma $ is a diagonal matrix with diagonal elements $(1,8)$. The data are then rotated by 45 degrees. (A) Decision boundaries from a single tree of RF. (B) The decision boundary obtained by projecting the input data onto the difference in class-conditional means. (C) The same as (B), but first passing the input to ranks before computing the difference in means.}}{3}{figure.1}}
\newlabel{fig:cigars}{{1}{3}{Scatter plot of parallel cigars in two dimensions. 1000 samples are obtained from multivariate Gaussians, where $\mu _j=[j, 0]$, for $j \in \{-1,1\}$, and $\Sigma $ is a diagonal matrix with diagonal elements $(1,8)$. The data are then rotated by 45 degrees. (A) Decision boundaries from a single tree of RF. (B) The decision boundary obtained by projecting the input data onto the difference in class-conditional means. (C) The same as (B), but first passing the input to ranks before computing the difference in means}{figure.1}{}}
\citation{Heath1993}
\citation{Trunk1979}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Evaluation}{4}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Axis Aligned Simulations}{4}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Classification performance comparing Random Forest (RF) to several variants of Randomer Forest (RerF), and Bayes optimal performance, on two distinct simulation settings: (A) Trunk and (B) Parity (see main text for details). For all settings, the left panels show a 2D scatter plot of the first 2 coordinates (dashed circles denote the standard deviation level set), the middle and right panels depict misclassification rate and training time against the number of ambient (coordinate) dimensions, respectively, RF to several variants of RerF, as well as the Bayes optimal performance. Note that in all settings, for all number of dimensions, RerF outperforms RF, even though these settings were designed specifically for RF because the discriminant boundary naturally lies along the coordinate basis. Although RerF requires slightly more time than RF (largely due to random sampling of projection matrices), they scale similarly. In each case, $n=100$ and the errorbars denote standard error over ten trials.}}{5}{figure.2}}
\newlabel{fig:sim}{{2}{5}{Classification performance comparing Random Forest (RF) to several variants of Randomer Forest (RerF), and Bayes optimal performance, on two distinct simulation settings: (A) Trunk and (B) Parity (see main text for details). For all settings, the left panels show a 2D scatter plot of the first 2 coordinates (dashed circles denote the standard deviation level set), the middle and right panels depict misclassification rate and training time against the number of ambient (coordinate) dimensions, respectively, RF to several variants of RerF, as well as the Bayes optimal performance. Note that in all settings, for all number of dimensions, RerF outperforms RF, even though these settings were designed specifically for RF because the discriminant boundary naturally lies along the coordinate basis. Although RerF requires slightly more time than RF (largely due to random sampling of projection matrices), they scale similarly. In each case, $n=100$ and the errorbars denote standard error over ten trials}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Theoretical Space and Time Complexity}{5}{subsection.4.2}}
\citation{Fisherfaces}
\citation{Delgado14}
\citation{Caruana2008}
\citation{Caruana2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Effects of Transformations and Outliers on Classifier Performance}{6}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Benchmark Data}{6}{subsection.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The effect of various transformations applied to the Trunk (panels A-D) and Parity (panels E-H) simulations (see Methods for details) on classification performance of (A,E) RF, (B,F) RerF (s+d), (C,G) RerF (s+d+r), and (D,H) Fisherfaces. Specifically, we consider, rotations, scalings, and affine transformations, as well as introducing outliers. Classification performance of RF is compromised by rotations and therefore affine transformations as well. RerF(s+d) is invariant to rotation, but not scaling and therefore not affine transformation. RerF(s+d+r) is invariant to to affine transformations. Like RerF (s+d), Fisherfaces is invariant to rotation but not scaling. Note that all variants are reasonably robust to outliers.}}{7}{figure.3}}
\newlabel{fig:invar}{{3}{7}{The effect of various transformations applied to the Trunk (panels A-D) and Parity (panels E-H) simulations (see Methods for details) on classification performance of (A,E) RF, (B,F) RerF (s+d), (C,G) RerF (s+d+r), and (D,H) Fisherfaces. Specifically, we consider, rotations, scalings, and affine transformations, as well as introducing outliers. Classification performance of RF is compromised by rotations and therefore affine transformations as well. RerF(s+d) is invariant to rotation, but not scaling and therefore not affine transformation. RerF(s+d+r) is invariant to to affine transformations. Like RerF (s+d), Fisherfaces is invariant to rotation but not scaling. Note that all variants are reasonably robust to outliers}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces (A) Classification error and training time of Random Forest and Randomer Forests variants for all 121 datasets from the benchmark comparison study by Caruana \cite  {Caruana2008}. (A) Average (dots) and 0.1 standard deviation level sets (dashed lines) for each method. (B) Classification error of Randomer Forest (sparse+delta+robust) vs. that of Random Forest for each of the 121 datasets. The black line indicates equal classification error of the two algorithms. Color indicates dimensionality of the datasets and size of points indicates number of samples. Note that RerF almost always does as well, and often significantly better.}}{7}{figure.4}}
\newlabel{fig:benchmark}{{4}{7}{(A) Classification error and training time of Random Forest and Randomer Forests variants for all 121 datasets from the benchmark comparison study by Caruana \cite {Caruana2008}. (A) Average (dots) and 0.1 standard deviation level sets (dashed lines) for each method. (B) Classification error of Randomer Forest (sparse+delta+robust) vs. that of Random Forest for each of the 121 datasets. The black line indicates equal classification error of the two algorithms. Color indicates dimensionality of the datasets and size of points indicates number of samples. Note that RerF almost always does as well, and often significantly better}{figure.4}{}}
\bibdata{nips2015_tyler}
\bibcite{Breiman2001}{1}
\bibcite{Delgado2014}{2}
\bibcite{Caruana2008}{3}
\bibcite{Heath1993}{4}
\bibcite{Menze2011}{5}
\bibcite{Tan2005}{6}
\bibcite{Ho1998}{7}
\bibcite{Rodriguez2006}{8}
\bibcite{Candes2009}{9}
\bibcite{Li2006}{10}
\bibcite{Trunk1979}{11}
\bibcite{Fisherfaces}{12}
\bibstyle{IEEEtran}
\@writefile{lof}{\contentsline {figure}{\numberline {A1}{\ignorespaces The number of trees until a stable misclassification rate is achieved for RF and the RerF variants in the Trunk, parity, and multimodal simulations. Simulation settings are the same as in Fig1. The number of ambient dimensions for panels A, B, and C were chosen to be 1000, 10, and 1000 respectively.}}{9}{figure.1}}
\newlabel{fig:ntrees}{{A1}{9}{The number of trees until a stable misclassification rate is achieved for RF and the RerF variants in the Trunk, parity, and multimodal simulations. Simulation settings are the same as in Fig1. The number of ambient dimensions for panels A, B, and C were chosen to be 1000, 10, and 1000 respectively}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A2}{\ignorespaces Classification performance comparing Random Forest (RF) to several variants of Randomer Forest (RerF) on the modified Trunk simulation setting. Here, the ith diagonal of the covariance matrices of both classes is equal to the inverse of the difference in means of the ith dimension between the two classes. For small values of p, all variants of RerF perform marginally better than RF. For large values of p, all variants of RerF perform worse than RF.}}{9}{figure.2}}
\newlabel{fig:trunk_hard}{{A2}{9}{Classification performance comparing Random Forest (RF) to several variants of Randomer Forest (RerF) on the modified Trunk simulation setting. Here, the ith diagonal of the covariance matrices of both classes is equal to the inverse of the difference in means of the ith dimension between the two classes. For small values of p, all variants of RerF perform marginally better than RF. For large values of p, all variants of RerF perform worse than RF}{figure.2}{}}
