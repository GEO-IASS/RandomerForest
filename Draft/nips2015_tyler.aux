\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Breiman2001}
\citation{Delgado2014}
\citation{Caruana2008}
\citation{Heath1993}
\citation{Heath1993}
\citation{Menze2011}
\citation{Heath1993}
\citation{Ho1998}
\citation{Rodriguez2006}
\citation{Menze2011}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Heath1993}
\citation{Menze2011}
\citation{Heath1993}
\citation{Tan2004}
\citation{Ho1998}
\citation{Rodriguez2006}
\citation{Menze2011}
\citation{Candes2009}
\citation{Candes2006}
\citation{Li2006}
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear Threshold Forests}{2}{section.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Psuedocode for Linear Threshold Forests, which generalizes a wide range of previously proposed decision forests.}}{3}{algorithm.1}}
\newlabel{pseudo}{{1}{3}{Linear Threshold Forests}{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Randomer Forests}{3}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Scatter plot of parallel cigars in two dimensions. 1000 samples are obtained from multivariate Gaussians, where $\mu _j=[j, 0]$, for $j \in \{-1,1\}$, and $\Sigma $ is a diagonal matrix with diagonal elements $(1,k)$ for $k$ is 4 (top row) or 8 (bottom row). The data are then rotated by 45 degrees. (A) Decision boundaries from a single tree of RF. (B) The decision boundary obtained by projecting the input data onto the difference in class-conditional means. (C) The same as (B), but first passing the input to ranks before computing the difference in means. (D), (E), and (F) are the same as (A), (B), and (C), respectively, for $k=8$.}}{3}{figure.1}}
\newlabel{fig:cigars}{{1}{3}{Scatter plot of parallel cigars in two dimensions. 1000 samples are obtained from multivariate Gaussians, where $\mu _j=[j, 0]$, for $j \in \{-1,1\}$, and $\Sigma $ is a diagonal matrix with diagonal elements $(1,k)$ for $k$ is 4 (top row) or 8 (bottom row). The data are then rotated by 45 degrees. (A) Decision boundaries from a single tree of RF. (B) The decision boundary obtained by projecting the input data onto the difference in class-conditional means. (C) The same as (B), but first passing the input to ranks before computing the difference in means. (D), (E), and (F) are the same as (A), (B), and (C), respectively, for $k=8$}{figure.1}{}}
\citation{Trunk1979}
\citation{Bickel2004}
\citation{Heath1993}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Simulated Data}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Evaluation}{4}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Axis Aligned Simulations}{4}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Classification performance comparing Random Forest (RF) to several variants of Randomer Forest (RerF), and Bayes optimal performance, on three distinct simulation settings: (A) Trunk, (B) Parity, and (C) Multimodal (see Methods for details). For all settings, the top panel shows a 2D scatter plot of the first 2 coordinates (dashed circles denote the standard deviation level set), the middle panel depicts misclassification rate vs. the number of ambient (coordinate) dimensions, and the bottom panel depicts training time comparing RF to several variants of RerF. Note that in all settings, for all number of dimensions, RerF outperforms RF, even for Trunk and Parity, which were designed specifically for RF because the discriminant boundary naturally lies along the coordinate basis. Although RerF requires slightly more time than RF (largely due to random sampling of projection matrices), they scale similarly. In each case, $n=100$ and the errorbars denote standard error over ten trials.}}{5}{figure.2}}
\newlabel{fig:sim}{{2}{5}{Classification performance comparing Random Forest (RF) to several variants of Randomer Forest (RerF), and Bayes optimal performance, on three distinct simulation settings: (A) Trunk, (B) Parity, and (C) Multimodal (see Methods for details). For all settings, the top panel shows a 2D scatter plot of the first 2 coordinates (dashed circles denote the standard deviation level set), the middle panel depicts misclassification rate vs. the number of ambient (coordinate) dimensions, and the bottom panel depicts training time comparing RF to several variants of RerF. Note that in all settings, for all number of dimensions, RerF outperforms RF, even for Trunk and Parity, which were designed specifically for RF because the discriminant boundary naturally lies along the coordinate basis. Although RerF requires slightly more time than RF (largely due to random sampling of projection matrices), they scale similarly. In each case, $n=100$ and the errorbars denote standard error over ten trials}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Theoretical Space and Time Complexity}{6}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Effects of Transformations and Outliers on Classifier Performance}{6}{subsection.4.3}}
\citation{Delgado2014}
\citation{Caruana2008}
\citation{Caruana2008}
\bibdata{nips2015_tyler}
\bibcite{Breiman2001}{1}
\bibcite{Delgado2014}{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Real Data}{7}{subsection.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{7}{section.5}}
\bibcite{Caruana2008}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The effect of various transformations applied to the Trunk (panels A-D) and Parity (panels E-H) simulations (see Methods for details) on classification performance of (A,E) RF, (B,F) RerF (s+d), (C,G) RerF (s+d+r), and (D,H) Fisherfaces. Specifically, we consider, rotations, scalings, and affine transformations, as well as introducing outliers. Classification performance of RF is compromised by rotations and therefore affine transformations as well. RerF(s+d) is invariant to rotation, but not scaling and therefore not affine transformation. RerF(s+d+r) is invariant to to affine transformations. Like RerF (s+d), Fisherfaces is invariant to rotation but not scaling. Note that all variants are reasonably robust to outliers.}}{8}{figure.3}}
\newlabel{fig:invar}{{3}{8}{The effect of various transformations applied to the Trunk (panels A-D) and Parity (panels E-H) simulations (see Methods for details) on classification performance of (A,E) RF, (B,F) RerF (s+d), (C,G) RerF (s+d+r), and (D,H) Fisherfaces. Specifically, we consider, rotations, scalings, and affine transformations, as well as introducing outliers. Classification performance of RF is compromised by rotations and therefore affine transformations as well. RerF(s+d) is invariant to rotation, but not scaling and therefore not affine transformation. RerF(s+d+r) is invariant to to affine transformations. Like RerF (s+d), Fisherfaces is invariant to rotation but not scaling. Note that all variants are reasonably robust to outliers}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces (A) Classification error and training time of Random Forest and Randomer Forests variants for all 121 datasets from the benchmark comparison study by Caruana \cite  {Caruana2008}. (A) Average (dots) and 0.1 standard deviation level sets (dashed lines) for each method. (B) Classification error of Randomer Forest (sparse+delta+robust) vs. that of Random Forest for each of the 121 datasets. The black line indicates equal classification error of the two algorithms. Color indicates dimensionality of the datasets and size of points indicates number of samples. Note that RerF almost always does as well, and often significantly better.}}{8}{figure.4}}
\newlabel{fig:4}{{4}{8}{(A) Classification error and training time of Random Forest and Randomer Forests variants for all 121 datasets from the benchmark comparison study by Caruana \cite {Caruana2008}. (A) Average (dots) and 0.1 standard deviation level sets (dashed lines) for each method. (B) Classification error of Randomer Forest (sparse+delta+robust) vs. that of Random Forest for each of the 121 datasets. The black line indicates equal classification error of the two algorithms. Color indicates dimensionality of the datasets and size of points indicates number of samples. Note that RerF almost always does as well, and often significantly better}{figure.4}{}}
\bibcite{Heath1993}{4}
\bibcite{Menze2011}{5}
\@writefile{lof}{\contentsline {figure}{\numberline {A1}{\ignorespaces The number of trees until a stable misclassification rate is achieved for RF and the RerF variants in the Trunk, parity, and multimodal simulations. Simulation settings are the same as in Fig1. The number of ambient dimensions for panels A, B, and C were chosen to be 1000, 10, and 1000 respectively.}}{9}{figure.1}}
\newlabel{fig:ntrees}{{A1}{9}{The number of trees until a stable misclassification rate is achieved for RF and the RerF variants in the Trunk, parity, and multimodal simulations. Simulation settings are the same as in Fig1. The number of ambient dimensions for panels A, B, and C were chosen to be 1000, 10, and 1000 respectively}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A2}{\ignorespaces Classification performance comparing Random Forest (RF) to several variants of Randomer Forest (RerF) on the modified Trunk simulation setting. Here, the ith diagonal of the covariance matrices of both classes is equal to the inverse of the difference in means of the ith dimension between the two classes. For small values of p, all variants of RerF perform marginally better than RF. For large values of p, all variants of RerF perform worse than RF.}}{9}{figure.2}}
\newlabel{fig:trunk_hard}{{A2}{9}{Classification performance comparing Random Forest (RF) to several variants of Randomer Forest (RerF) on the modified Trunk simulation setting. Here, the ith diagonal of the covariance matrices of both classes is equal to the inverse of the difference in means of the ith dimension between the two classes. For small values of p, all variants of RerF perform marginally better than RF. For large values of p, all variants of RerF perform worse than RF}{figure.2}{}}
\bibcite{Ho1998}{6}
\bibcite{Rodriguez2006}{7}
\bibcite{Candes2009}{8}
\bibcite{Trunk1979}{9}
\bibstyle{IEEEtran}
